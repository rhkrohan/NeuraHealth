{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyPWXp0cpuiXpp87xGiLJPcS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# ============================================================\n","# 0 Â· Mount Google Drive\n","# ============================================================\n","from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VYOOmjcqV7Ld","executionInfo":{"status":"ok","timestamp":1745734507558,"user_tz":240,"elapsed":21080,"user":{"displayName":"Aura","userId":"05343723233220360391"}},"outputId":"e546a268-9435-47a0-cd32-5d02e40e19e6"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["\n","# ============================================================\n","# 1 Â· Safe-runtime setup\n","# ============================================================\n","import os, pathlib, datetime, tensorflow as tf, h5py   # h5py needed for .h5 checkpoints\n","\n","# â†’ Use GPU if Colab gave you one, otherwise stay on CPU\n","if not tf.config.list_physical_devices(\"GPU\"):\n","    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n","    BATCH_SIZE = 8          # lighter for CPU\n","    print(\"â†’ Running on CPU\")\n","else:\n","    BATCH_SIZE = 16\n","    print(\"â†’ GPU detected:\", tf.config.list_physical_devices(\"GPU\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UPv6T8hBV9Ba","executionInfo":{"status":"ok","timestamp":1745734513561,"user_tz":240,"elapsed":4408,"user":{"displayName":"Aura","userId":"05343723233220360391"}},"outputId":"938bed2e-e27f-4a87-aafb-46f23f3dbd74"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["â†’ GPU detected: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"]}]},{"cell_type":"code","source":["\n","# ============================================================\n","# 2 Â· Point to the extracted dataset on Drive\n","#    (edit the path if you stored it elsewhere)\n","# ============================================================\n","DATA_DIR = pathlib.Path(\"/content/drive/MyDrive/chest_xray\")\n","\n","print(\"\\nSanity-check image counts:\")\n","for split in [\"train\", \"val\", \"test\"]:\n","    for cls in [\"NORMAL\", \"PNEUMONIA\"]:\n","        n = len(list((DATA_DIR / split / cls).glob(\"*\")))\n","        print(f\"{split}/{cls:9s}: {n}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fWXpCLI8WAzX","executionInfo":{"status":"ok","timestamp":1745734536840,"user_tz":240,"elapsed":21273,"user":{"displayName":"Aura","userId":"05343723233220360391"}},"outputId":"08716277-d10c-4dac-a46e-4a096669baa6"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Sanity-check image counts:\n","train/NORMAL   : 1341\n","train/PNEUMONIA: 3875\n","val/NORMAL   : 8\n","val/PNEUMONIA: 8\n","test/NORMAL   : 234\n","test/PNEUMONIA: 390\n"]}]},{"cell_type":"code","source":["\n","# ============================================================\n","# 3 Â· Letter-box preprocessing (keeps aspect ratio)\n","#     Returned array must be NumPy because ImageDataGenerator\n","# ============================================================\n","IMG_SIZE = 160\n","\n","import numpy as np\n","def preprocess_fn(img):\n","    \"\"\"img uint8 HÃ—WÃ—C  â†’  float32 320Ã—320Ã—3\"\"\"\n","    img = img.astype(\"float32\") / 255.0\n","    h, w, _ = img.shape\n","    scale   = IMG_SIZE / max(h, w)\n","    nh, nw  = int(round(h*scale)), int(round(w*scale))\n","    img_rs  = tf.image.resize(img, (nh, nw)).numpy()\n","\n","    canvas  = np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)\n","    y0, x0  = (IMG_SIZE-nh)//2, (IMG_SIZE-nw)//2\n","    canvas[y0:y0+nh, x0:x0+nw, :] = img_rs\n","    return canvas"],"metadata":{"id":"DQ4QbjcTWEmo","executionInfo":{"status":"ok","timestamp":1745734536843,"user_tz":240,"elapsed":2,"user":{"displayName":"Aura","userId":"05343723233220360391"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras import layers, models, applications, optimizers, callbacks\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","# -----------------------------------------------------------\n","# define the generators (must come BEFORE flow_from_directory)\n","# -----------------------------------------------------------\n","train_gen = ImageDataGenerator(\n","    preprocessing_function = preprocess_fn,\n","    shear_range            = 0.2,\n","    zoom_range             = 0.2,\n","    horizontal_flip        = True)\n","\n","test_gen  = ImageDataGenerator(preprocessing_function = preprocess_fn)\n","\n","# -----------------------------------------------------------\n","# now you can create the three datasets\n","# -----------------------------------------------------------\n","train_ds = train_gen.flow_from_directory(\n","    DATA_DIR / \"train\",\n","    target_size = (IMG_SIZE, IMG_SIZE),\n","    batch_size  = BATCH_SIZE,\n","    class_mode  = 'binary')\n","\n","val_ds   = test_gen.flow_from_directory(\n","    DATA_DIR / \"val\",\n","    target_size = (IMG_SIZE, IMG_SIZE),\n","    batch_size  = BATCH_SIZE,\n","    class_mode  = 'binary')\n","\n","test_ds  = test_gen.flow_from_directory(\n","    DATA_DIR / \"test\",\n","    target_size = (IMG_SIZE, IMG_SIZE),\n","    batch_size  = BATCH_SIZE,\n","    class_mode  = 'binary')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TV-wx7J_2STn","executionInfo":{"status":"ok","timestamp":1745734540731,"user_tz":240,"elapsed":165,"user":{"displayName":"Aura","userId":"05343723233220360391"}},"outputId":"88c86d8f-9fc7-4c7a-ede1-3e43ac83022d"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5216 images belonging to 2 classes.\n","Found 16 images belonging to 2 classes.\n","Found 624 images belonging to 2 classes.\n"]}]},{"cell_type":"code","source":["# â–¼ Replace the whole \"model + fit\" section with this\n","# ============================================================\n","#  Fast MobileNetV2 baseline  Â·  input 160Ã—160\n","# ============================================================\n","IMG_SIZE   = 160         # down from 320\n","BATCH_SIZE = 32          # can be larger at this resolution\n","EPOCHS     = 8           # warm-up only (frozen backbone)\n","\n","# Re-create the generators at smaller size\n","train_ds = train_gen.flow_from_directory(\n","    DATA_DIR / \"train\", target_size=(IMG_SIZE, IMG_SIZE),\n","    batch_size=BATCH_SIZE, class_mode='binary')\n","val_ds   = test_gen.flow_from_directory(\n","    DATA_DIR / \"val\",   target_size=(IMG_SIZE, IMG_SIZE),\n","    batch_size=BATCH_SIZE, class_mode='binary')\n","test_ds  = test_gen.flow_from_directory(\n","    DATA_DIR / \"test\",  target_size=(IMG_SIZE, IMG_SIZE),\n","    batch_size=BATCH_SIZE, class_mode='binary')\n","\n","# ---------- model ----------\n","\n","base = applications.MobileNetV2(\n","    include_top=False, weights='imagenet',\n","    input_shape=(IMG_SIZE, IMG_SIZE, 3), pooling='avg')\n","base.trainable = False                 # freeze ALL conv layers\n","\n","model = models.Sequential([\n","    base,\n","    layers.Dropout(0.3),\n","    layers.Dense(1, activation='sigmoid')\n","])\n","\n","model.compile(\n","    optimizer = optimizers.Adam(1e-3),\n","    loss      = 'binary_crossentropy',\n","    metrics   = ['accuracy', tf.keras.metrics.AUC(name='auroc')]\n",")\n","\n","model.summary()\n","\n","# ---------- callbacks ----------\n","import datetime, pathlib, os, h5py\n","stamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n","ckpt  = callbacks.ModelCheckpoint(\n","            f\"/content/drive/MyDrive/models/mnv2_{stamp}.h5\",\n","            save_best_only=True, monitor='val_auroc', mode='max')\n","early = callbacks.EarlyStopping(\n","            monitor='val_auroc', mode='max',\n","            patience=2, restore_best_weights=True)\n","\n","# ---------- train ----------\n","history = model.fit(\n","    train_ds,\n","    validation_data = val_ds,\n","    epochs          = EPOCHS,\n","    callbacks       = [ckpt, early]\n",")\n","\n","# ---------- evaluate ----------\n","print(\"\\nğŸ”  Test-set performance\")\n","model.evaluate(test_ds)\n","\n","# optional: unfreeze last 20 layers & fine-tune 3 more epochs\n","# (adds ~5 min; skip if you just need a quick baseline)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":626},"id":"vCyZ0aZ9WIWy","executionInfo":{"status":"ok","timestamp":1745736234620,"user_tz":240,"elapsed":1691175,"user":{"displayName":"Aura","userId":"05343723233220360391"}},"outputId":"9827c635-ab96-4c43-c57a-82c1ab7fe881"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5216 images belonging to 2 classes.\n","Found 16 images belonging to 2 classes.\n","Found 624 images belonging to 2 classes.\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_160_no_top.h5\n","\u001b[1m9406464/9406464\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n","â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n","â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n","â”‚ mobilenetv2_1.00_160            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           â”‚     \u001b[38;5;34m2,257,984\u001b[0m â”‚\n","â”‚ (\u001b[38;5;33mFunctional\u001b[0m)                    â”‚                        â”‚               â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚         \u001b[38;5;34m1,281\u001b[0m â”‚\n","â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n","â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n","â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n","â”‚ mobilenetv2_1.00_160            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> â”‚\n","â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    â”‚                        â”‚               â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> â”‚\n","â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,259,265\u001b[0m (8.62 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,259,265</span> (8.62 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,281\u001b[0m (5.00 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> (5.00 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/8\n","\u001b[1m163/163\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.7874 - auroc: 0.8135 - loss: 0.4461"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m163/163\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m942s\u001b[0m 6s/step - accuracy: 0.7878 - auroc: 0.8141 - loss: 0.4453 - val_accuracy: 0.8750 - val_auroc: 0.9531 - val_loss: 0.2766\n","Epoch 2/8\n","\u001b[1m163/163\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 497ms/step - accuracy: 0.9103 - auroc: 0.9682 - loss: 0.2039"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m163/163\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 504ms/step - accuracy: 0.9103 - auroc: 0.9682 - loss: 0.2038 - val_accuracy: 0.8750 - val_auroc: 0.9844 - val_loss: 0.2747\n","Epoch 3/8\n","\u001b[1m163/163\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 500ms/step - accuracy: 0.9275 - auroc: 0.9762 - loss: 0.1754 - val_accuracy: 0.9375 - val_auroc: 0.9688 - val_loss: 0.2411\n","Epoch 4/8\n","\u001b[1m163/163\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 497ms/step - accuracy: 0.9394 - auroc: 0.9823 - loss: 0.1500 - val_accuracy: 0.9375 - val_auroc: 0.9688 - val_loss: 0.2239\n","\n","ğŸ”  Test-set performance\n","\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m370s\u001b[0m 18s/step - accuracy: 0.8188 - auroc: 0.9095 - loss: 0.4313\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.3650846779346466, 0.8477563858032227, 0.9291694164276123]"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["model.save('/pneumonia.keras');"],"metadata":{"id":"ASn_zanLYOxv","executionInfo":{"status":"ok","timestamp":1745736273029,"user_tz":240,"elapsed":455,"user":{"displayName":"Aura","userId":"05343723233220360391"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["model.save('/pneumonia.h5');\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ny-JkK_c-gAr","executionInfo":{"status":"ok","timestamp":1745736274579,"user_tz":240,"elapsed":241,"user":{"displayName":"Aura","userId":"05343723233220360391"}},"outputId":"f73c8382-1ae3-484b-8ed2-1864b7bbd24c"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]}]},{"cell_type":"code","source":["import numpy as np\n","from PIL import Image\n","import tensorflow as tf\n","\n","def prepare_image(path: str, img_size: int = 160) -> np.ndarray:\n","    \"\"\"\n","    Load an image from disk, keep its aspect-ratio, centre-pad to a square\n","    canvas of `img_size`Ã—`img_size`, and return a float32 NumPy array\n","    scaled to [0,1].\n","\n","    Parameters\n","    ----------\n","    path : str\n","        Path to a JPG / PNG / etc.\n","    img_size : int, default 160\n","        Target size expected by the model.\n","\n","    Returns\n","    -------\n","    np.ndarray\n","        Array of shape (img_size, img_size, 3), dtype float32, values in [0,1].\n","    \"\"\"\n","    # 1. read â†’ RGB\n","    img = Image.open(path).convert(\"RGB\")\n","    w, h = img.size\n","\n","    # 2. resize with unchanged aspect ratio (shorter side = img_size)\n","    scale = img_size / max(w, h)\n","    nw, nh = int(round(w * scale)), int(round(h * scale))\n","    img = img.resize((nw, nh), resample=Image.BILINEAR)\n","\n","    # 3. letter-box (black padding) to square canvas\n","    canvas = Image.new(\"RGB\", (img_size, img_size), (0, 0, 0))\n","    canvas.paste(img, ((img_size - nw) // 2, (img_size - nh) // 2))\n","\n","    # 4. to float32 [0,1]\n","    arr = np.asarray(canvas, dtype=\"float32\") / 255.0\n","    return arr\n","\n","\n","prepare_image('/content/drive/MyDrive/chest_xray/test/PNEUMONIA/person100_bacteria_475.jpeg')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B5typgdvsmkc","executionInfo":{"status":"ok","timestamp":1745731909608,"user_tz":240,"elapsed":2412,"user":{"displayName":"Aura","userId":"05343723233220360391"}},"outputId":"ce844ea5-7fbe-4827-92dd-cfd2c5efa62f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 0.],\n","        ...,\n","        [0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 0.]],\n","\n","       [[0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 0.],\n","        ...,\n","        [0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 0.]],\n","\n","       [[0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 0.],\n","        ...,\n","        [0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 0.]],\n","\n","       ...,\n","\n","       [[0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 0.],\n","        ...,\n","        [0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 0.]],\n","\n","       [[0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 0.],\n","        ...,\n","        [0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 0.]],\n","\n","       [[0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 0.],\n","        ...,\n","        [0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 0.]]], dtype=float32)"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["from tensorflow.keras.models import load_model\n","\n","model = load_model(\"/content/pneumonia.keras\")\n","\n","path = '/content/drive/MyDrive/chest_xray/test/PNEUMONIA/person100_bacteria_475.jpeg'\n","\n","def predict_image(path):\n","    x = prepare_image(path, img_size=160)\n","    x = np.expand_dims(x, axis=0)       # batch dimension\n","    prob = model.predict(x)[0][0]       # sigmoid output\n","    label = \"PNEUMONIA\" if prob >= 0.5 else \"NORMAL\"\n","    return {\"probability\": float(prob), \"prediction\": label}\n","\n","\n","predict_image(path)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FuUGRNhss7sN","executionInfo":{"status":"ok","timestamp":1745731961022,"user_tz":240,"elapsed":6532,"user":{"displayName":"Aura","userId":"05343723233220360391"}},"outputId":"cae63605-50cb-49ca-b90f-8abaf407523b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 6 variables whereas the saved optimizer has 2 variables. \n","  saveable.load_own_variables(weights_store.get(inner_path))\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step\n"]},{"output_type":"execute_result","data":{"text/plain":["{'probability': 0.966402530670166, 'prediction': 'PNEUMONIA'}"]},"metadata":{},"execution_count":6}]}]}